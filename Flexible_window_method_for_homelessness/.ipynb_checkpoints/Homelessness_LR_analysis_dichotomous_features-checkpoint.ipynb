{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a846a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm \n",
    "from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, classification_report, roc_auc_score, f1_score, precision_score, recall_score\n",
    "from plot_metric.functions import BinaryClassification\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pd.options.display.max_rows = 200\n",
    "\n",
    "def Find_Optimal_Cutoff(target, predicted):    \n",
    "    '''Find data-driven cut-off for classification\n",
    "    \n",
    "    Cut-off is determined using Youden's index defined as sensitivity + specificity - 1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    target : array, shape = [n_samples]\n",
    "        True binary labels.\n",
    "        \n",
    "    predicted : array, shape = [n_samples]\n",
    "        Target scores, can either be probability estimates of the positive class,\n",
    "        confidence values, or non-thresholded measure of decisions (as returned by\n",
    "        “decision_function” on some classifiers).\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    Ewald, B. (2006). Post hoc choice of cut points introduced bias to diagnostic research.\n",
    "    Journal of clinical epidemiology, 59(8), 798-801.\n",
    "    \n",
    "    Steyerberg, E.W., Van Calster, B., & Pencina, M.J. (2011). Performance measures for\n",
    "    prediction models and markers: evaluation of predictions and classifications.\n",
    "    Revista Espanola de Cardiologia (English Edition), 64(9), 788-794.\n",
    "    \n",
    "    Jiménez-Valverde, A., & Lobo, J.M. (2007). Threshold criteria for conversion of probability\n",
    "    of species presence to either–or presence–absence. Acta oecologica, 31(3), 361-369.\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(target, predicted)\n",
    "    idx = np.argmax(tpr - fpr)\n",
    "    return thresholds[idx]\n",
    "\n",
    "def train_model(data_final, day):\n",
    "    # List of features for training\n",
    "    feature_list_for_training = [\n",
    "           'sex_M',\n",
    "           'age_18-29',  'age_40-49', 'age_50-59', 'age_60+',\n",
    "           'substance', \n",
    "           'mood', \n",
    "           'anxiety', \n",
    "           'psychotic', \n",
    "           'cognitive', \n",
    "           'otherpsych',\n",
    "           'selfharm', \n",
    "           'visit_emr_MH_non_elect', \n",
    "           'visit_emr_NonMH',\n",
    "           'visit_emr_visit', \n",
    "           'visit_hosp_visit',\n",
    "           'visit_hospitalized_MH', \n",
    "           'visit_hospitalized_NonMH', \n",
    "           'visit_family_gp', \n",
    "           'visit_im',\n",
    "           'visit_neurology', \n",
    "           'visit_other', \n",
    "           'visit_pharmacy', \n",
    "           'visit_psychiatry',\n",
    "           'EX_CHF', \n",
    "           'EX_Arrhy', \n",
    "           'EX_VD', \n",
    "           'EX_PCD', \n",
    "           'EX_PVD', 'EX_HPTN_UC',\n",
    "           'EX_HPTN_C', 'EX_Para', 'Ex_OthND', 'Ex_COPD', 'Ex_Diab_UC',\n",
    "           'Ex_Diab_C', 'Ex_Hptothy', 'Ex_RF', 'Ex_LD', 'Ex_PUD_NB', 'Ex_HIV',\n",
    "           'Ex_Lymp', 'Ex_METS', 'Ex_Tumor', 'Ex_Rheum_A', 'Ex_Coag', 'Ex_Obesity',\n",
    "           'Ex_WL', 'Ex_Fluid', 'Ex_BLA', 'Ex_DA', 'Ex_Alcohol', 'Ex_Drug',\n",
    "           'Ex_Psycho', 'Ex_Dep', 'Ex_Stroke', 'Ex_Dyslipid', 'Ex_Sleep', 'Ex_IHD',\n",
    "           'EX_Fall', 'EX_Urinary', 'EX_Visual', 'EX_Hearing', 'EX_Tobacco',\n",
    "           'EX_Delirium', 'Ex_MS', 'EX_parkinsons'\n",
    "    ]\n",
    "    \n",
    "    # Separate the features for training\n",
    "    X = data_final[feature_list_for_training]\n",
    "    # Separate the labels for training\n",
    "    y = data_final['homeless']\n",
    "    \n",
    "    # Add the constant to the X features\n",
    "    X = sm.add_constant(X)\n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "    # Fit the logistic regression model\n",
    "    log_reg = sm.Logit(y_train, X_train).fit()\n",
    "\n",
    "    # After fitting the model, we need to test the model on X_test (yhat are the predicted probabilities)\n",
    "    yhat = log_reg.predict(X_test)\n",
    "    # Receive the labels from yhat for the classification\n",
    "    # Find optimal probability threshold by using yhat as a continuous measure\n",
    "    threshold = Find_Optimal_Cutoff(y_test, yhat)\n",
    "\n",
    "    # Find predictions to the dataframe applying the threshold\n",
    "    prediction = pd.Series(yhat).map(lambda x: 1 if x > threshold else 0)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    f1 = f1_score(y_test, prediction)\n",
    "    roc_auc = roc_auc_score(y_test, prediction)\n",
    "    sensitivity = recall_score(y_test, prediction)\n",
    "    precision = precision_score(y_test, prediction)\n",
    "    number_of_records = len(data_final)\n",
    "    number_ind_with_first_outcome = data_final[data_final['homeless'] == 1].count()[0]\n",
    "\n",
    "    res = {\n",
    "        'f1': round(f1, 2),\n",
    "        'roc_auc': round(roc_auc, 2),\n",
    "        'sensitivity': round(sensitivity, 2),\n",
    "        'precision': round(precision, 2),\n",
    "        'days': day,\n",
    "        'number_ind_with_first_outcome': number_ind_with_first_outcome,\n",
    "        'number_of_records': number_of_records\n",
    "    }\n",
    "\n",
    "#     print(day, number_of_records, number_ind_with_first_outcome)\n",
    "    \n",
    "    return res, X_train, X_test, y_train, y_test, prediction, log_reg, feature_list_for_training, X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ea74d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the preprocessed data from an HDF file\n",
    "data_final = pd.read_hdf('df_sub_homeless_preproc.h5', 'df_sub_homeless_preproc')\n",
    "\n",
    "# Create empty DataFrames to store the results\n",
    "df_results = pd.DataFrame()\n",
    "df_ = pd.DataFrame()\n",
    "\n",
    "# Define a list of time intervals\n",
    "list_ = [0, 30, 60, 90, 180, 360, 720]\n",
    "\n",
    "# Iterate over each time interval\n",
    "for day in list_:\n",
    "    # Filter the data based on the date difference\n",
    "    data_final_ = data_final[data_final['date_difference'] >= timedelta(day)]\n",
    "    \n",
    "    try:\n",
    "        # Train the model and get the results\n",
    "        res, X_train, X_test, y_train, y_test, prediction, log_reg, feature_list_for_training, X, y = train_model(data_final_, day)\n",
    "        \n",
    "        # Concatenate the results to the df_results DataFrame\n",
    "        df_results = pd.concat([df_results, pd.DataFrame(res.values()).T], ignore_index=True)\n",
    "    except:\n",
    "        # Print the day if an exception occurs during training\n",
    "        print(day)\n",
    "\n",
    "# Rename the columns of the df_results DataFrame\n",
    "df_results.columns = ['f1', 'roc_auc', 'sensitivity', 'precision', 'day', 'number_ind_with_first_outcome', 'number_of_records']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(['roc_auc', 'sensitivity', 'precision'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the time interval for multivariate analysis\n",
    "day = 360\n",
    "\n",
    "# Filter the data based on the date difference\n",
    "data_final_ = data_final[data_final['date_difference'] >= timedelta(day)]\n",
    "\n",
    "# Train the model and get the results for the specified time interval\n",
    "res, X_train, X_test, y_train, y_test, prediction, log_reg, feature_list_for_training, X, y = train_model(data_final_, day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b1edb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate and print the confusion matrix for the test dataset\n",
    "cm = confusion_matrix(y_test, prediction)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Calculate and print the accuracy score of the model\n",
    "print('Test accuracy =', accuracy_score(y_test, prediction))\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, prediction))\n",
    "\n",
    "# Calculate and print the F1 score\n",
    "print(\"F1 Score:\", f1_score(y_test, prediction))\n",
    "\n",
    "# Calculate and print the AUC-ROC score\n",
    "roc_auc = roc_auc_score(y_test, prediction)\n",
    "print('AUC: %.2f' % roc_auc + \"%\")\n",
    "\n",
    "# Visualize the ROC curve using plot_metric\n",
    "bc = BinaryClassification(y_test, prediction, labels=[\"Class 1\", \"Class 2\"])\n",
    "\n",
    "# Create a figure and plot the ROC curve\n",
    "plt.figure(figsize=(5, 5))\n",
    "bc.plot_roc_curve()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Set the time interval for multivariate analysis\n",
    "day = 360\n",
    "\n",
    "# Filter the data based on the time interval\n",
    "data_final = data_final[data_final['date_difference'] > timedelta(day)]\n",
    "\n",
    "# Retrieve the results from the logistic regression model\n",
    "result = log_reg\n",
    "\n",
    "# Calculate the confidence intervals and odds ratios\n",
    "df_results = np.round(np.exp(result.conf_int()), 2)\n",
    "df_results.columns = ['CI:2.5%', 'CI:97.5%']\n",
    "df_results['OR'] = pd.DataFrame(np.exp(result.params))\n",
    "df_results['OR'] = df_results['OR'].map(lambda x: np.round(x, 2))\n",
    "df_results['P_value'] = np.round(result.pvalues.values, 2)\n",
    "\n",
    "# Sort the results by odds ratio in descending order\n",
    "df_results_sorted = df_results[['OR', 'CI:2.5%', 'CI:97.5%', 'P_value']].sort_values(\"OR\", ascending=False)\n",
    "\n",
    "df_results_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a61708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
